{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with Python and Keras\n",
    "## \"Alice's Adventures in Wonderland\" by Lewis Carroll\n",
    "\n",
    "### What Is A LSTM?\n",
    "\n",
    "From https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "\n",
    "> Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n",
    "\n",
    "![LSTM Diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/300px-Peephole_Long_Short-Term_Memory.svg.png \"LSTM Diagram\")\n",
    "\n",
    "\n",
    "\n",
    "### Generating Text With LSTMs\n",
    "\n",
    "Below we will write a script, that uses these powerful LSTMs to generate text. The first step is importing all the required libraries. You will need python 3.x (I'm using 3.6.2) installed with keras (2.2.0) and numpy (1.14.4) set up.\n",
    "\n",
    " - https://www.python.org/\n",
    " - https://www.tensorflow.org/install/\n",
    "\n",
    "After installing required dependencies, install Keras with pip\n",
    "\n",
    "`pip install keras`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167497"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the required libraries\n",
    "import urllib.request\n",
    "import keras\n",
    "from keras import Input\n",
    "from keras.callbacks import LambdaCallback, ReduceLROnPlateau\n",
    "from keras.engine import Model\n",
    "from keras.layers import Dense, Activation, Dropout, GRU, Concatenate, concatenate, regularizers, LSTM\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/dakrone/corpus/master/data/alice-in-wonderland.txt'\n",
    "with urllib.request.urlopen(path) as response:\n",
    "    text = response.read().decode(\"utf-8\").lower()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Text\n",
    "We have created a quick tokenizer, which will split the text into words. There is also some code to split punctuation marks into their own words. The class provided is not too important to dive deeply into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "class OneHotScaler:\n",
    "    \"\"\"\n",
    "        Converts the corpus into \"word\" vectors. One Hot encoding\n",
    "\n",
    "        Note that text should have proper UTF quotation marks.\n",
    "        Otherwise you may need to change this class a bit (untested, see specials below)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_indices = None\n",
    "        self.indices_char = None\n",
    "        self.allowed = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\",.“”‘’!?…-;/():'\n",
    "        self.specials = '\",.“”‘’!?…\\';():'  # These get separated into their own token\n",
    "\n",
    "    def fit_transform(self, text):\n",
    "        self.fit(text)\n",
    "        return self.transform(text)\n",
    "\n",
    "    def transform(self, text):\n",
    "        # pre-filter text\n",
    "        # collapse multi newline into one newline\n",
    "        text = text.replace('\\n', ' ')\n",
    "        token_text = ''.join([c for c in text if c in self.allowed])\n",
    "        # lets split it by words (text, )\n",
    "        # first lets make punctuation marks separate words\n",
    "        for special in self.specials:\n",
    "            token_text = token_text.replace(special, ' {} '.format(special))\n",
    "        words = [w for w in token_text.split(' ') if w != '']\n",
    "        idx_arr = np.array([self.char_indices[w] for w in words], dtype='int64')\n",
    "        one_hot = np.eye(len(self.char_indices))[idx_arr]\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, text):\n",
    "        # pre-filter text\n",
    "        # collapse multi newline into one newline\n",
    "        text = text.replace('\\n', ' ')\n",
    "        token_text = ''.join([c for c in text if c in self.allowed])\n",
    "        # lets split it by words (text, )\n",
    "        # first lets make punctuation marks separate words\n",
    "        for special in self.specials:\n",
    "            token_text = token_text.replace(special, ' {} '.format(special))\n",
    "        words = [w for w in token_text.split(' ') if w != '']\n",
    "        chars = sorted(list(set(words)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    def inverse_transform(self, vec_list, temperature=1.0):\n",
    "        l1 = [self.indices_char[sample(vec, temperature)] for vec in vec_list]\n",
    "        reconstr = \"\".join([\" {}\".format(i) if i not in string.punctuation else i for i in l1]).strip()\n",
    "        return reconstr.replace(' ’ ', '’').replace('“ ', '\\n\\n“').replace(' ”', '”')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Data\n",
    "We use a \"One Hot\" method, to encode words into vectors, the dimensions of the vector will be the size of the dictionary. Each vector will be of magnitude 1.0.\n",
    "\n",
    "A small example, if a dictionary looks like so:\n",
    "\n",
    "```\n",
    "dictionary = [\"The\", \"owl\", \"ate\"]\n",
    "```\n",
    "\n",
    "The word \"The\" will be converted to a vector.\n",
    "\n",
    "```\n",
    "wordvec_the = [1.0, 0.0, 0.0]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([ 0.,  0.,  0., ...,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 5\n",
    "# skip over n sequences. 1 = do not skip over any sequences\n",
    "step = 1\n",
    "# how many words to generate after each epoch of training\n",
    "generate = 250\n",
    "\n",
    "b = OneHotScaler()\n",
    "# Convert the text into an array of vectors. Each vector represents a word\n",
    "one_hot = b.fit_transform(text)\n",
    "\n",
    "# Convert above array, into an array of sequences.\n",
    "x = np.array([one_hot[i: i + maxlen] for i in range(0, len(one_hot) - maxlen, step)])\n",
    "y = np.array([one_hot[i + maxlen] for i in range(0, len(one_hot) - maxlen, step)])\n",
    "\n",
    "# Lets take a look at the data. X is an array of input vectors, Y is the desired output we train to predict\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Keras Model\n",
    "We build a simple, one layer LSTM network. We specify the input shape from variables used above.\n",
    "The LSTM takes that same shape, and uses 128 hidden cells. Return sequences must be sent to False in order to \n",
    "return a flat array, not an array of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 5, 3278)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               1744384   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 3278)              422862    \n",
      "=================================================================\n",
      "Total params: 2,167,246\n",
      "Trainable params: 2,167,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a Keras model\n",
    "inputs = Input(shape=(maxlen, x.shape[2]))\n",
    "lstm_long = LSTM(128, input_shape=(maxlen, x.shape[2]), return_sequences=False)(inputs)\n",
    "lstm_long = Dropout(0.2)(lstm_long)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(y.shape[1], activation='softmax', name='main_output')(lstm_long)\n",
    "\n",
    "# Using the Adam optimizer\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "model = Model(inputs=inputs, outputs=main_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Which Runs on Each \"Epoch\"\n",
    "Each \"epoch\", we run a function. The logic is a bit over-engineered probably, but it allows us to be general and can be modified for other ways to represent the input data. For now, we are just using the \"word tokenization\" method we worked through above.\n",
    "\n",
    "The logic, is that it selects a random string of words from the corpus, of length `maxlen`, and uses that as a \"seed\" phrase to generate more text, and allow its imagination run wild. Each iteration, the generated word is tagged on the end of the seed, and the first word is removed, and a new prediction is made. This runs `generate` times which is a variable specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, one_hot.shape[0] - maxlen - 1)\n",
    "    for diversity in [1.0, ]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        sentence = np.zeros((generate+maxlen, y.shape[1]))\n",
    "        for n in range(maxlen):\n",
    "            sentence[n, :] = np.copy(one_hot[start_index+n, :])\n",
    "\n",
    "        print('----- Generating with seed: \"{}\"'.format(b.inverse_transform(sentence[0:maxlen], diversity)))\n",
    "\n",
    "        for i in range(generate):\n",
    "            model_input = np.zeros((1, maxlen, y.shape[1]))\n",
    "            for n in range(maxlen):\n",
    "                model_input[0, n, :] = np.copy(sentence[n+i, :])\n",
    "\n",
    "            preds = model.predict(model_input, verbose=0)[0]\n",
    "            char = b.inverse_transform(np.array([preds, ]), diversity)\n",
    "            sentence[i+maxlen] = np.array(b.transform(char)[0])\n",
    "\n",
    "        print(b.inverse_transform(np.array(sentence), diversity))\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Text\n",
    "\n",
    "Now we just call the fit function, and generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "34560/34584 [============================>.] - ETA: 0s - loss: 6.2908 - acc: 0.0728----- Generating text after Epoch: 0\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"so it was indeed:\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so it was indeed: did out state some is, in that ill looked) lobsters rather to alice rose said set at things queens of at lieu with deny head, of she put said cheshire from she king and at said seemed and\" to party which grin you? to back king said she into you shes. went who bird other, these meekly the. ill distributing her frog all obliged middle visit her end lobsters this nurse jurymen thinking whisper all there, here once butter snail. centre that alice your queen dogs then--i that invented at offended silence know the hatter two cant up you watch paragraph the of as outside, his three and know--and used. this ground been dog she works said cut she no got and a will nothing poor to the and way queen know a manage simply: much my for put the look the its lobsters who into this curious to lory\" but a alice after the she with down to like the -- out elegant clubs last to; the their the more. i, this half shower said! eyes jury master of back great has look their remarked which drink whole without all youth visit soon, about and all sat. to things including said said itll to, be hid old! kept dance see it have and: only the, into officers their meaning about the he instantly some can\n",
      "34584/34584 [==============================] - 84s - loss: 6.2908 - acc: 0.0728    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1202e0b8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback, ], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Can We Improve This?\n",
    "\n",
    "In order to improve there are many things that can be done\n",
    "\n",
    "- You should definitely run for more epochs. I have only run 1, you should run at least 40.\n",
    "- Edit the corpus, and remove identifiers and non-necessary text like credits, etc.\n",
    "- Use a different word representation, like Word2Vec\n",
    "- Modify the network to use more layers and / or more hidden layers\n",
    "- Add some more features to the input. One idea I had was \"in_quote\" feature, 1.0 for inside a quote, and 0.0 for outside\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example after 20 Epochs\n",
    "\n",
    "We can see for the most part, the grammer is getting pretty good! Try it out yourself and let me know how you do!\n",
    "\n",
    "\n",
    "> your verdict, he said to the jury, in a low, trembling voice, like majesty said: the hatter. i hurried forgotten it, a white moment day like of deep exactly grown she did, she was a right mind i had then half it? the pieces: interrupted the dormouse, been, and hold and there to see, it would did your mostly she had going to them to herself, but she did not dropped to she began off. but when so serpents down, while being could pinched never afraid i must be managing; thought she said quite eyes; but she thought he, but she began them at last in sea case of first gloves dropped cats! i dont dont her him the whiting shouldnt came about, said alice. dr cats down to, said alice. over only first opened a just out a think! ive went! in the politely tone, she said to herself. the gardeners so found, my her without--maybe morning! we give set much to day. it had read are in a undertone down: the came man and two about, and would not hardly maintaining it must rustled. so it was a very french to some of the last time, and nothing suddenly running up going too. becoming a taken zigzag in soup, that she had considering with as looking twist at her,( three-legged neck; but second\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}